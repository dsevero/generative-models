{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsevero/generative-models/blob/master/experiments/GAN/notebooks/goodfellow_2014.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69jryAXYtlmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVCHBrRRuUUY",
        "colab_type": "text"
      },
      "source": [
        "# Generative Adversarial Nets (Goodfellow 2014)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfJmN-YMroqt",
        "colab_type": "text"
      },
      "source": [
        "**References:**\n",
        "* https://arxiv.org/pdf/1406.2661.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0j0DEAPANpt",
        "colab_type": "text"
      },
      "source": [
        "## Autograd\n",
        "(doesn't work very well :/, training time is harsh maybe using jax will suffice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8SOx2KGriTc",
        "colab_type": "text"
      },
      "source": [
        "**References:** \n",
        "* https://github.com/HIPS/autograd/blob/master/examples/generative_adversarial_net.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyDcYCjnhb0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1pQTYwsDlAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPDM2dRnt5gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# necessary for autograd compatibility\n",
        "!pip install scipy==1.1.0 -q\n",
        "\n",
        "# update tensorflow\n",
        "!pip install tensorflow==2.0.0b1 -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5INWa4setf75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import autograd.numpy as np\n",
        "import autograd.numpy.random as npr\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "from autograd.misc import flatten\n",
        "from autograd.misc.optimizers import adam, rmsprop, sgd\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSdGVpxdD-Aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_image(image):\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p756YZQnvFZp",
        "colab_type": "text"
      },
      "source": [
        "### Definitions and Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIimoT6YsJER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(x):       \n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):    \n",
        "    return 0.5 * (np.tanh(x) + 1.0)\n",
        "\n",
        "def logsigmoid(x): \n",
        "    return x - np.logaddexp(0, x)\n",
        "\n",
        "def init_random_params(scale, layer_sizes, rs=npr.RandomState(0)):\n",
        "    \"\"\"Build a list of (weights, biases) tuples,\n",
        "       one for each layer in the net.\"\"\"\n",
        "    return [(scale * rs.randn(m, n),   # weight matrix\n",
        "             scale * rs.randn(n))      # bias vector\n",
        "            for m, n in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
        "\n",
        "def batch_normalize(activations):\n",
        "    mbmean = np.mean(activations, axis=0, keepdims=True)\n",
        "    return (activations - mbmean) / (np.std(activations, axis=0, keepdims=True) + 1)\n",
        "\n",
        "def neural_net_predict(params, inputs, use_batch_norm=True):\n",
        "    \"\"\"Params is a list of (weights, bias) tuples.\n",
        "       inputs is an (N x D) matrix.\"\"\"\n",
        "    inpW, inpb = params[0]\n",
        "    inputs = relu(np.dot(inputs, inpW) + inpb)\n",
        "    for W, b in params[1:-1]:\n",
        "        outputs = np.dot(inputs, W) + b\n",
        "        if use_batch_norm:\n",
        "            outputs = batch_normalize(outputs)\n",
        "        inputs = relu(outputs)\n",
        "    outW, outb = params[-1]\n",
        "    outputs = np.dot(inputs, outW) + outb\n",
        "    return outputs\n",
        "\n",
        "def generator(params, noise):\n",
        "    samples = neural_net_predict(params, noise)\n",
        "    return sigmoid(samples)\n",
        "\n",
        "def discriminator(params, inputs):\n",
        "    samples = neural_net_predict(params, inputs)\n",
        "    return sigmoid(samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DA1eNmWvM4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = 5*((np.eye(3), np.zeros(3)),)\n",
        "x =  npr.randn(9).reshape((3,3))\n",
        "zeros = np.array([0, 0, 0])\n",
        "\n",
        "assert (batch_normalize(zeros) == zeros).all()\n",
        "assert (batch_normalize(np.array([-1, 1])) == np.array([-0.5, 0.5])).all()\n",
        "assert (batch_normalize(np.array([[-1, 1], [-2, 2]])) == np.array([[1/3, -1/3], [-1/3, 1/3]])).all()\n",
        "\n",
        "assert (neural_net_predict(params, x, False) == relu(x)).all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7buAOgGK6FuJ",
        "colab_type": "text"
      },
      "source": [
        "### 5 Experiments (MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLYwGW7U6YqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "param_scale = 0.001\n",
        "batch_size = 1_000 # must divide 70_000.\n",
        "num_epochs = 10\n",
        "num_batches = int(70_000/batch_size)\n",
        "\n",
        "\n",
        "# Load, concatenate and reshape the data.\n",
        "# Here we don't distinguish between train and test.\n",
        "# Final dimensions are (batch, image_example, image_row, image_column)\n",
        "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
        "images = (np.concatenate((train_images, test_images))\n",
        "            .reshape((num_batches, batch_size, 28, 28))) / 255\n",
        "\n",
        "# Model hyper-parameters\n",
        "noise_dim = 100\n",
        "gen_layer_sizes = [noise_dim, 50, 50, 784]\n",
        "dsc_layer_sizes = [gen_layer_sizes[-1], 50, 50, 1]\n",
        "\n",
        "# Initialize NN params. for G and D\n",
        "init_gen_params = init_random_params(param_scale, gen_layer_sizes)\n",
        "init_dsc_params = init_random_params(param_scale, dsc_layer_sizes)\n",
        "\n",
        "print('images.shape:', images.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h15wslYoDSjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Real image\n",
        "show_image(images[0, 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTx70Qo9Gwil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initial random image from G\n",
        "z = npr.rand(1, noise_dim)\n",
        "fake_image = generator(init_gen_params, z)\n",
        "show_image(fake_image.reshape(28, 28))\n",
        "\n",
        "# What D knows about the random image (i.e. nothing!)\n",
        "discriminator(init_dsc_params, fake_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R4Kj-_ZNcri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Even real images he has no clue on what is going on!\n",
        "five_real_images = images[0, :5]\n",
        "discriminator(init_dsc_params, five_real_images.reshape(5, 28*28))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJQ2aop0Ol2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train\n",
        "n_dsc_steps = 1\n",
        "params_dsc = init_dsc_params\n",
        "opt = rmsprop\n",
        "\n",
        "def objective_outer(params_gen, t):\n",
        "    global params_dsc\n",
        "    real_data = images[t % num_batches].reshape(batch_size, 28*28)\n",
        "    \n",
        "    def objective_inner(params_dsc, t):\n",
        "        noise_inner = npr.rand(batch_size, noise_dim)\n",
        "        fake_data_inner = generator(params_gen, noise_inner)\n",
        "        return(np.log(discriminator(params_dsc, real_data)).mean() + \n",
        "               np.log(1 - discriminator(params_dsc, fake_data_inner))).mean()\n",
        "        \n",
        "    params_dsc = opt(grad(objective_inner), params_dsc,\n",
        "                     step_size=0.01, num_iters=n_dsc_steps)\n",
        "    \n",
        "    noise_outer = npr.rand(batch_size, noise_dim)\n",
        "    fake_data_outer = generator(params_gen, noise_outer)\n",
        "    return np.log(discriminator(params_dsc, fake_data_outer)).mean()\n",
        "\n",
        "z = npr.rand(1, noise_dim)\n",
        "def callback(params_gen, t, gradient):    \n",
        "    print(f'Iteration {t};')\n",
        "#     print(f'z.mean() = {z.mean()}')\n",
        "#     print(f'params_gen = {params_gen[0][0].mean()}')\n",
        "#     print(f'params_dsc = {params_dsc[0][0].mean()}')\n",
        "    \n",
        "    fake_image = generator(params_gen, z)\n",
        "    show_image(fake_image.reshape(28, 28))\n",
        "\n",
        "params_gen = opt(grad(objective_outer), \n",
        "                 init_gen_params,\n",
        "                 step_size=0.1,\n",
        "                 num_iters=num_epochs*num_batches,\n",
        "                 callback=callback)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXrojyobAUOt",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLQ7CstOrPVh",
        "colab_type": "text"
      },
      "source": [
        "**References:**\n",
        "* https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py\n",
        "* https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK4a5WkfAWQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "1ef98c2c-78b0-43ed-e8b5-2596d4843fdb"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"GPU Available? =\", torch.cuda.is_available())\n",
        "print(\"Modules versions:\")\n",
        "!pip freeze | grep -E '^numpy==|^pandas==|^torch==|^torchvision=='"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Available? = True\n",
            "Modules versions:\n",
            "numpy==1.16.4\n",
            "pandas==0.24.2\n",
            "torch==1.1.0\n",
            "torchvision==0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSnhXv3Qy_tS",
        "colab_type": "text"
      },
      "source": [
        "### Generator & Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eol7ToP7sAUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, img_shape):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img\n",
        "\n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8skqgpwzEhz",
        "colab_type": "text"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQnUX8Cns8Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = True if torch.cuda.is_available() else False\n",
        "latent_dim = 100\n",
        "batch_size = 2**16\n",
        "img_size = 28\n",
        "img_shape = (1, img_size, img_size)\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_epochs = 200\n",
        "sample_interval = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFk_9YtXzQkA",
        "colab_type": "text"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6aLwtZXzWWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MNIST_PATH = Path('/content/drive/data/mnist')\n",
        "os.makedirs(MNIST_PATH, exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        MNIST_PATH,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.Resize(img_size), \n",
        "             transforms.ToTensor(), \n",
        "             transforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeSMJeVyzaVD",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnMxOCOOy8ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize generator and discriminator\n",
        "generator = Generator(latent_dim, img_shape)\n",
        "discriminator = Discriminator(img_shape)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(),\n",
        "                               lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), \n",
        "                               lr=lr, betas=(b1, b2))\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "if cuda:\n",
        "    print('Will use GPU')\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    Tensor = torch.cuda.FloatTensor\n",
        "else:\n",
        "    print('Will NOT use GPU')\n",
        "    Tensor = torch.FloatTensor\n",
        "\n",
        "IMAGES_PATH = Path('/content/drive/data/images6')\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), \n",
        "                         requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), \n",
        "                        requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "        )\n",
        "        if batches_done % sample_interval == 0:\n",
        "            save_image(gen_imgs.data[:25], \n",
        "                       IMAGES_PATH / f\"{batches_done}.png\" , \n",
        "                       nrow=5, \n",
        "                       normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}